---
title: "code_illustration"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

######################
## Simulation study ##
######################

## Simulation setting
```{r}
rm(list = ls())
source('mSIM_illustration.R')
## construct functions
# 1.non-linear setting
fun.list = list()
fun.list[[1]] = function(x){return(x)}
fun.list[[2]] = function(x){return(x^2)}
fun.list[[3]] = function(x){return(x^3)}
fun.list[[4]] = function(x){return(sin(x))}
fun.list[[5]] = function(x){return(atan(x))}
fun.list[[6]] = function(x){return(exp(x))}
fun.list[7:12] = fun.list[13:18] = fun.list[1:6]

# 2.linear setting
#fun.list = list()
#fun.list[[1]] = function(x){return(x)}
#fun.list[2:18] = fun.list[1]

## construct coefficient matrix
snr = 4 # set signal to noise ratio
n = 200; p = 100; q=length(fun.list); cor = 0 # set n, p, correlation
signal = p*0.06
B.true = matrix(0, p, q)
alpha1 = c(rep(1,6),rep(2,6),rep(3,6))
for(i in 1:(signal/3)){
  B.true[i,] = alpha1
}
alpha2 = c(rep(2,6),rep(3,6),rep(1,6))
for(i in (signal/3+1):(2*signal/3)){
  B.true[i,] = alpha2
}
alpha3 = c(rep(3,6),rep(1,6),rep(2,6))
for(i in (2*signal/3+1):(3*signal/3)){
  B.true[i,] = alpha3
}
B.true = col.norm(B.true) # true rank = 3
```

## Run for a single pair of (lambda, rank) as an example
```{r}
data = get.data(fun.list, n, p, B.true, snr, cor)
Y = data$y.scaled
X = data$x
B.init = get.B.ridge(data$y.scaled, data$x, lambda=0.01)
test = get.B.ADMM(Y=Y, X=X, B=B.init, lambda=0.2, rank=3, alpha=1, control1 = list(max.iter=5e1, tol=1e-5), control2=list(ele.sparse=F, row.sparse=T, low.rank=T), select.method='linear', plot=F)
B.final = test$B.final
cat('converge: ', test$converge, '\n') 
cat('TPR: ', 100*sum(B.final[1:signal,]!=0) / (signal*q), '\n')
cat('FPR: ', 100*sum(B.final[-(1:signal),]!=0) / ((p-signal)*q), '\n')
cat('MSE for par est: ', sum((B.final[1:signal,]-B.true[1:signal,])^2) / (signal*q), '\n')

## Diagnostic plot
plot(x=1:length(test$pri.err), y=test$pri.err, type='l', xlab='Iteration', ylab='Pri Err')
par(mfrow=c(2,3))
for(i in 1:6){
  get.plot.raw(y=Y[,i], x=X, beta=B.final[,i], ylab=i, linear=T)
}
```

## Run simulation in parallel example
```{r, eval=F}
## tuning parameters
Data = list()
B.init = list()
simulation = 1

## generate data
for(simu in 1:simulation){
  set.seed(simu^2 + simu*1000)
  Data[[simu]] = data = get.data(fun.list, n, p, B.true, snr, cor)
  B.init[[simu]] = get.B.ridge(data$y.scaled, data$x, lambda=0.01)
}
range = c(0.25, 0.75)
rank = c(3, 10)
tuning = list() # all combination of tuning parameters
len.tuning = 0
for(i in rank){
  for(j in range){
    len.tuning = len.tuning + 1
    tuning[[len.tuning]] = c(j, i)
  }
} 
ncores = 4 
cl = makeCluster(ncores)
registerDoParallel(cl)
B.simu = foreach(simu=1:simulation, .packages=c('mgcv','Matrix','splines','gtools')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  model.admm = list()
  
  # mSIM model
  temp = list()
  for(i in 1:length(tuning)){
    temp[[i]] = get.B.ADMM(Y=Y, X=X, B=B.init[[simu]], lambda=tuning[[i]][1], rank=tuning[[i]][2], alpha=1, control1=list(max.iter=5e1, tol=1e-3), select.method='linear', plot=F)
  }
  model.admm[[1]] = NULL

  # B for each tuning parameter
  B = list()
  for(i in 1:length(tuning)){
    B[[i]] = temp[[i]]$B.sparse
  }
  model.admm[[2]] = B
  
  # BIC for each tuning parameter
  BIC.msim = NULL
  try(for(i in 1:length(tuning)){
    #print(i)
    BIC.msim = rbind(BIC.msim, B.BIC(Y=Y, X=X, B=B[[i]], tuning=tuning[[i]], linear=F))
  })
  model.admm[[3]] = BIC.msim

  return(model.admm)
}
stopCluster(cl)
```

## Comprehensive simulation using cluster; do NOT run it without a cluster 
```{r, eval=F}
library(doSNOW)

Data = list()
B.init = list()
#simulation = 100
simulation = 1

## generate data
for(simu in 1:simulation){
  set.seed(simu^2 + simu*1000)
  Data[[simu]] = data = get.data(fun.list, n, p, B.true, snr, cor)
  B.init[[simu]] = get.B.ridge(data$y.scaled, data$x, lambda=0.01)
}

## parallel for tuning parameters
ncores = 32
range = exp(seq(-4,1,length.out=10))
rank = 1:6

tuning = list()
len.tuning = 0
for(i in rank){
  for(j in range){
    len.tuning = len.tuning + 1
    tuning[[len.tuning]] = c(j, i)
  }
}

cl = makeCluster(ncores)
registerDoSNOW(cl)

B.NSR = foreach(simu=1:simulation, .packages=c('mgcv','Matrix','splines','gtools')) %dopar% {
  set.seed(simu^2 + 100*simu + 1)
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  model.admm = list()
  
  # NSR model
  temp = list()
  for(i in 1:length(tuning)){
    temp[[i]] = get.B.ADMM(Y=Y, X=X, B=B.init[[simu]], lambda=tuning[[i]][1], rank=tuning[[i]][2], alpha=1, control1=list(max.iter=1e2, tol=1e-5), control2=list(ele.sparse=F, row.sparse=T, low.rank=T), select.method='linear', descent.method='bfgs', plot=F)
  }
  model.admm[[1]] = NULL
  
  # B for each tuning parameter
  B = list()
  for(i in 1:length(tuning)){
    B[[i]] = temp[[i]]$B.sparse
  }
  model.admm[[2]] = B
  
  # BIC for each tuning parameter
  BIC.msim = NULL
  try(for(i in 1:length(tuning)){
    #print(i)
    BIC.msim = rbind(BIC.msim, B.BIC.new(Y=Y, X=X, B=B[[i]], tuning=tuning[[i]], linear=F))
  })
  model.admm[[3]] = BIC.msim
  
  
  return(model.admm)
}

B.NS = foreach(simu=1:simulation, .packages=c('mgcv','Matrix','splines','gtools')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  model.admm = list()
  
  # SR model
  temp = list()
  for(i in 1:length(range)){
    temp[[i]] = get.B.ADMM(Y=Y, X=X, B=B.init[[simu]], lambda=range[[i]], rank=18, alpha=1, control1=list(max.iter=1e2, tol=1e-5), control2=list(ele.sparse=F, row.sparse=T, low.rank=F), select.method='linear', descent.method='bfgs', plot=F)
  }
  model.admm[[1]] = NULL
  
  # B for each tuning parameter
  B = list()
  for(i in 1:length(range)){
    B[[i]] = temp[[i]]$B.sparse
  }
  model.admm[[2]] = B
  
  # BIC for each tuning parameter
  BIC.msim = NULL
  try(for(i in 1:length(range)){
    #print(i)
    BIC.msim = rbind(BIC.msim, B.BIC.new(Y=Y, X=X, B=B[[i]], tuning=c(range[[i]], 18), linear=F))
  })
  model.admm[[3]] = BIC.msim
  
  
  return(model.admm)
}

B.NR = foreach(simu=1:simulation, .packages=c('mgcv','Matrix','splines','gtools')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  model.admm = list()
  
  # NR model
  temp = list()
  for(i in 1:length(rank)){
    temp[[i]] = get.B.ADMM(Y=Y, X=X, B=B.init[[simu]], lambda=0, rank=rank[[i]], alpha=1, control1=list(max.iter=1e2, tol=1e-5), control2=list(ele.sparse=F, row.sparse=F, low.rank=T), select.method='linear', descent.method='bfgs', plot=F)
  }
  model.admm[[1]] = NULL
  
  # B for each tuning parameter
  B = list()
  for(i in 1:length(rank)){
    B[[i]] = temp[[i]]$B.sparse
  }
  model.admm[[2]] = B
  
  # BIC for each tuning parameter
  BIC.msim = NULL
  try(for(i in 1:length(tuning)){
    #print(i)
    BIC.msim = rbind(BIC.msim, B.BIC.new(Y=Y, X=X, B=B[[i]], tuning=c(0, rank[[i]]), linear=F))
  })
  model.admm[[3]] = BIC.msim
  
  
  return(model.admm)
}

B.SR = foreach(simu=1:simulation, .packages=c('rrpack')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  rtn = list()
  
  fit = cv.srrr(Y,X,nrank=3,method=c('adglasso'))
  rtn[[1]] = fit$coef
  rtn[[2]] = fit$rank
  
  return(rtn)
}

B.S = foreach(simu=1:simulation, .packages=c('glmnet')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  
  fit = cv.glmnet(x=X, y=Y, family='mgaussian', intercept=F)
  temp = NULL
  for(i in 1:q){
    temp = cbind(temp, coef(fit, s="lambda.min")[[i]][-1])
  }
  
  return(temp)
}

B.R = foreach(simu=1:simulation, .packages=c('rrpack')) %dopar% {
  data = Data[[simu]]
  Y=data$y.scaled
  X=data$x
  rtn = list()
  
  fit = cv.rrr(Y,X)
  rtn[[1]] = fit$coef
  rtn[[2]] = fit$rank
  
  return(rtn)
}

stopCluster(cl)

## calculate metrics
summary = matrix(,simulation,12)
mspe = matrix(,simulation,6)
Data.new = list()

for(simu in 1:simulation){
  set.seed(simu)
  data = Data[[simu]]
  data.new = Data.new[[simu]] = get.data(fun.list, 50*n, p, B.true, snr, cor)
  Y=data$y.scaled
  X=data$x
  Y.true = Data.new[[simu]]$y.scaled
  X.true = Data.new[[simu]]$x
  
  best.NSR = which.min(B.NSR[[simu]][[3]][,2])
  summary[simu, 1] = sum(B.NSR[[simu]][[2]][[best.NSR]][1:signal, ] != 0) / (q*signal)
  summary[simu, 2] = sum(B.NSR[[simu]][[2]][[best.NSR]][-(1:signal), ] != 0) / (q*(p-signal))
  summary[simu, 3] = tuning[[best.NSR]][2]
  mspe[simu, 1] = msim.pred(Y=Y, X=X, B=B.NSR[[simu]][[2]][[best.NSR]], Y.true=data.new$y.scaled, X.pred=data.new$x)$MSE
  
  best.NS = which.min(B.NS[[simu]][[3]][,2])
  summary[simu, 4] = sum(B.NS[[simu]][[2]][[best.NS]][1:signal, ] != 0) / (q*signal)
  summary[simu, 5] = sum(B.NS[[simu]][[2]][[best.NS]][-(1:signal), ] != 0) / (q*(p-signal))
  mspe[simu, 2] = msim.pred(Y=Y, X=X, B=B.NS[[simu]][[2]][[best.NS]], Y.true=data.new$y.scaled, X.pred=data.new$x)$MSE
  
  best.NR = which.min(B.NR[[simu]][[3]][,2])
  summary[simu, 6] = tuning[[best.NR]][2]
  mspe[simu, 3] = msim.pred(Y=Y, X=X, B=B.NR[[simu]][[2]][[best.NR]], Y.true=data.new$y.scaled, X.pred=data.new$x)$MSE

  summary[simu, 7] = sum(B.SR[[simu]][[1]][1:signal, ] != 0) / (q*signal)
  summary[simu, 8] = sum(B.SR[[simu]][[1]][-(1:signal), ] != 0) / (q*(p-signal))
  summary[simu, 9] = B.SR[[simu]][[2]]
  mspe[simu, 4] = sum((Y.true - X.true%*%B.SR[[simu]][[1]])^2) / prod(dim(Y.true))

  summary[simu, 10] = sum(B.S[[simu]][1:signal, ] != 0) / (q*signal)
  summary[simu, 11] = sum(B.S[[simu]][-(1:signal), ] != 0) / (q*(p-signal))
  mspe[simu, 5] = sum((Y.true - X.true%*%B.S[[simu]])^2) / prod(dim(Y.true))
  
  summary[simu, 12] = B.R[[simu]][[2]]
  mspe[simu, 6] = sum((Y.true - X.true%*%B.R[[simu]][[1]])^2) / prod(dim(Y.true))
} 
```



########################
## Real data analysis ##
########################

## A example of MSIM on real data with (lambda=0.03877, rank=5)
```{r}
rm(list = ls())
source('new.R')
load('realdata.Rdata')
X = scale(X) # standardize by our assumption
Y = scale(Y)
B.init = get.B.ridge(Y=Y, X=X, lambda=0.01)
real.MSIM = get.B.ADMM(Y=Y, X=X, B=B.init, lambda=0.03877, rank=5, alpha=1, control1 = list(max.iter=1e5, tol=1e-4), control2=list(ele.sparse=F, row.sparse=T, low.rank=T), select.method='linear', plot=F)
cat('converge: ', real.MSIM$converge, '\n')
plot(x=1:length(real.MSIM$pri.err), y=real.MSIM$pri.err, type='l', xlab='Iteration', ylab='Pri Err')
cat('Percentage of covariates selected: ', 100*sum(real.MSIM$B.final[,1]!=0) / dim(X)[2], '\n')
decomp = svd(real.MSIM$B.final)
cat('Non-zero singular values are : ', round(decomp$d[decomp$d > 1e-8], 2), '\n')
par(mfrow=c(2,3))
for(i in 1:6){
  get.plot.raw(y=Y[,i], x=X, beta=real.MSIM$B.final[,i], ylab=colnames(Y)[i])
}
```

## Comparison of different model with noise added; do NOT run it without a cluster 
```{r, eval=F}
fold = 10
set.seed(222)
X = cbind(X, matrix(rnorm(161*118, sd=1), nrow=118, ncol=161)) # add noise
cv_ind = permute(rep(1:fold, ceiling(dim(Y)[1]/fold)))[1:dim(Y)[1]]
result_MSE = matrix(NA, fold, 4)
result_TPR = matrix(NA, fold, 4)
result_FPR = matrix(NA, fold, 4)
range = exp(seq(-3, -0.5, by=0.2))
rank = seq(1,15,by=2)
tuning = list() 
len.tuning = 0
for(i in rank){
  for(j in range){
    len.tuning = len.tuning + 1
    tuning[[len.tuning]] = c(j, i)
  }
} 

for(k in 1:fold){
  cat('fold: ', k, '\n')
  choosen = (cv_ind==k)
  Y.train = Y[!choosen,]
  X.train = X[!choosen,]
  Y.test = Y[choosen,]
  X.test = X[choosen,]
  
  if(1){
    B.init = get.B.ridge(Y.train, X.train, lambda=0.01)
    ncores = min(detectCores()-1, len.tuning) 
    cl = makeCluster(ncores)
    registerDoParallel(cl)
    B.msim = foreach(i=1:length(tuning), .packages=c('mgcv','Matrix','splines','gtools')) %dopar% {
      model.admm = list()
      # tuning the mSIM model
      temp = try(get.B.ADMM(Y=Y.train, X=X.train, B=B.init, lambda=tuning[[i]][1], rank=tuning[[i]][2], alpha=1, control1=list(max.iter=5e1, tol=1e-4), control2=list(row.sparse=T, low.rank=T), select.method='linear', plot=F))
      return(temp)
    }
    stopCluster(cl)
    
    tbl = NULL
    for(i in 1:length(tuning)){
      tbl = rbind(tbl, B.BIC(Y=Y.train, X=X.train, B=B.msim[[i]]$B.final, tuning=tuning[[i]], linear=F))
    }
    cat('par', tuning[[which.min(tbl[,3])]], '\n') # select the best tuing parameter combination
    
    B.final = B.msim[[which.min(tbl[,3])]]$B.final
    Y.msim = msim.pred(Y=Y.train, X=X.train, B=B.final, X.pred=X.test)
    result_TPR[k,1] = sum(B.final[1:39,]!=0)/(39*62)
    result_FPR[k,1] = sum(B.final[40:200,]!=0)/(161*62)
    result_MSE[k,1] = sum((Y.test-Y.msim)^2) / prod(dim(Y.test))
  }
  
  if(1){
    model.rrr = rrr(Y.train, X.train, penaltySVD=c('rank'), modstr=list(lambda=rank))
    cat('optimal rank for rrr is: ', model.rrr$rank, '\n')
    result_TPR[k,2] = sum(coef(model.rrr)[1:39,]!=0)/(39*62)
    result_MSE[k,2] = sum((X.test %*% coef(model.rrr) - Y.test)^2) / prod(dim(Y.test))
  }
  
  if(1){
    model.srrr = srrr(Y.train, X.train, method=c('glasso'), modstr=list(lamA=range), nrank=model.rrr$rank)
    result_TPR[k,3] = sum(coef(model.srrr)[1:39,]!=0)/(39*62)
    result_FPR[k,3] = sum(coef(model.srrr)[40:200,]!=0)/(161*62)
    result_MSE[k,3] = sum((X.test %*% coef(model.srrr) - Y.test)^2) / prod(dim(Y.test))
  }
  
  if(1){
    model.mlin = cv.glmnet(x=X.train, y=Y.train, lambda=range, family=c('mgaussian'), intercept=F)
    B.mlin = NULL
    for(i in 1:dim(B.final)[2]){
      B.mlin = cbind(B.mlin, coef(model.mlin)[[i]])
    }
    B.mlin = B.mlin[-1,]
    result_TPR[k,4] = sum(B.mlin[1:39,]!=0)/(39*62)
    result_FPR[k,4] = sum(B.mlin[40:200,]!=0)/(161*62)
    result_MSE[k,4] = sum((X.test %*% B.mlin - Y.test)^2) / prod(dim(Y.test))
  }
}

cat('MSE', '\n')
print(result_MSE)
cat('TPR', '\n')
print(result_TPR)
cat('FPR', '\n')
print(result_FPR)

```



